<!DOCTYPE html>
<html lang="zh"><head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>深度学习学习笔记——数据操作</title>
    <meta charset="utf-8">
    <meta name="description" content="Ladder@主要学习网站：https://zh.d2l.ai/
b站课程视频链接：https://space.bilibili.com/1567748478/channel/seriesdetail?sid=358497
视频和网站配合学习，一开始没看到有b站课程，只看网站学看不懂函数是干什么的，还不会问gpt，只是自己硬啃，确实有点吃力。。。
这里我学习的是tensorflow框架，并在jupyter中进行编写
前言什么的略过，主要记录代码方面学习到的内容。">
    <meta name="author" content="p1um">
    <link rel="canonical" href="https://p1um3r.github.io/blog/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C/">

    <link rel="alternate" type="application/rss+xml" href="https://p1um3r.github.io/index.xml" title="p1um&#39;s blog">

    


    <meta property="og:title" content="深度学习学习笔记——数据操作" />
<meta property="og:description" content="主要学习网站：https://zh.d2l.ai/
b站课程视频链接：https://space.bilibili.com/1567748478/channel/seriesdetail?sid=358497
视频和网站配合学习，一开始没看到有b站课程，只看网站学看不懂函数是干什么的，还不会问gpt，只是自己硬啃，确实有点吃力。。。
这里我学习的是tensorflow框架，并在jupyter中进行编写
前言什么的略过，主要记录代码方面学习到的内容。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://p1um3r.github.io/blog/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C/" /><meta property="article:section" content="blog" />
<meta property="article:published_time" content="2023-12-17T18:06:00+08:00" />
<meta property="article:modified_time" content="2023-12-17T18:06:00+08:00" />



<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="深度学习学习笔记——数据操作"/>
<meta name="twitter:description" content="主要学习网站：https://zh.d2l.ai/
b站课程视频链接：https://space.bilibili.com/1567748478/channel/seriesdetail?sid=358497
视频和网站配合学习，一开始没看到有b站课程，只看网站学看不懂函数是干什么的，还不会问gpt，只是自己硬啃，确实有点吃力。。。
这里我学习的是tensorflow框架，并在jupyter中进行编写
前言什么的略过，主要记录代码方面学习到的内容。"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Blogs",
      "item": "https://p1um3r.github.io/blog/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "深度学习学习笔记——数据操作",
      "item": "https://p1um3r.github.io/blog/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "深度学习学习笔记——数据操作",
  "name": "深度学习学习笔记——数据操作",
  "description": "主要学习网站：https://zh.d2l.ai/\nb站课程视频链接：https://space.bilibili.com/1567748478/channel/seriesdetail?sid=358497\n视频和网站配合学习，一开始没看到有b站课程，只看网站学看不懂函数是干什么的，还不会问gpt，只是自己硬啃，确实有点吃力。。。\n这里我学习的是tensorflow框架，并在jupyter中进行编写\n前言什么的略过，主要记录代码方面学习到的内容。",
  "keywords": [
    
  ],
  "articleBody": "主要学习网站：https://zh.d2l.ai/\nb站课程视频链接：https://space.bilibili.com/1567748478/channel/seriesdetail?sid=358497\n视频和网站配合学习，一开始没看到有b站课程，只看网站学看不懂函数是干什么的，还不会问gpt，只是自己硬啃，确实有点吃力。。。\n这里我学习的是tensorflow框架，并在jupyter中进行编写\n前言什么的略过，主要记录代码方面学习到的内容。\n1、导入tensorflow import tensorflow as tf 2、创建向量 x = tf.range(12) x 3、判断形状 x.shape TensorShape([12]) 4、判断元素数量 tf.size(x) \u003ctf.Tensor: shape=(), dtype=int32, numpy=12\u003e 5、改变张量形状 X = tf.reshape(x, (3, 4)) X \u003ctf.Tensor: shape=(3, 4), dtype=int32, numpy= array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], dtype=int32)\u003e 6、生成符合正态分布的随机取样(均值为0、标准差为1) tf.random.normal(shape=[3, 4]) 7、生成定值张量 tf.constant([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]) 8、基本运算 对于任意具有相同形状的张量， 常见的标准算术运算符（+、-、*、/和**）都可以被升级为按元素运算。\n还有tf.exp(x)也是按元素计算\n9、张量连结(concatenate) X = tf.reshape(tf.range(12, dtype=tf.float32), (3, 4)) Y = tf.constant([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]) tf.concat([X, Y], axis=0), tf.concat([X, Y], axis=1) (,\r) 使用$tf.concat()$函数来实现，其中$[X,Y]$分别表示需要连结的两个张量，$X$在前，$Y$在后，参数$axis$代表的是在某个指定方向上进行连接，$0$为行，$1$为列\n由于例子中使用的是二维张量，故只有$0$和$1$两个方向。若为三维张量，则还可能沿着$2$方向进行连结。\n10、求和 tf.reduce_sum(X) \u003ctf.Tensor: shape=(), dtype=float32, numpy=66.0\u003e 机制 1、广播机制 主要用于两个形状不同的张量进行按元素操作。\n注意：\n形状可以不同，但维度必须相同，维度不同的两个张量不可以通过广播机制来进行按元素操作\n工作方式：\n通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状； 对生成的数组执行按元素操作。 在大多数情况下，我们将沿着数组中长度为1的轴进行广播，如下例子：\nMXNET PYTORCH TENSORFLOW PADDLE a = tf.reshape(tf.range(3), (3, 1)) b = tf.reshape(tf.range(2), (1, 2)) a, b (,\r) 这里$a$和$b$分别是$3×1$和$1×2$的矩阵，为了让二者相加，我们将两个矩阵广播为形状为$3×2$的矩阵，方法为：将矩阵$a$复制列，矩阵$b$复制行，然后再按元素相加。\na + b 我们可以使用来查看广播后的结果：\ntf.broadcast_to(a, [3, 2]), tf.broadcast_to(b, [3, 2]) (,\r) 2、索引和切片 与python索引和切片机制相同，0为第一个元素，-1为最后一个元素。\n由于TensorFlow中的Tensors是不可变的，也不能被赋值，我们可以使用Variables来生成一个可以赋值的可变容器。==请记住，TensorFlow中的梯度不会通过Variable反向传播（？看不懂）==。\n方法如下：\nX_var = tf.Variable(X) X_var[1, 2].assign(9) X_var ",
  "wordCount" : "510",
  "inLanguage": "zh",
  "datePublished": "2023-12-17T18:06:00+08:00",
  "dateModified": "2023-12-17T18:06:00+08:00",
  "author":{
    "@type": "Person",
    "name": "p1um"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://p1um3r.github.io/blog/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "p1um's blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://p1um3r.github.io/favicon.ico"
    }
  }
}
</script>
    <link rel="icon" href="/images/avatar.jpg" sizes="16x16">

<link rel="apple-touch-icon" href="/images/avatar.jpg">

<link rel="manifest" href="/images/avatar.jpg">

    <link rel="stylesheet" href="https://cdn.staticfile.org/lxgw-wenkai-webfont/1.6.0/style.css" />

    
    
    
    <link rel="stylesheet" href="/css/main.min.bcddc5727486fc24236ed42c762caad768389f0c18142a88f210b37dc057dfaf.css" integrity="sha256-vN3FcnSG/CQjbtQsdiyq12g4nwwYFCqI8hCzfcBX368=" crossorigin="anonymous" media="screen" />
    


    
    <link rel="stylesheet" href="/scss/highlight/github-dark.min.min.66034289ee9a113219a2c4aae0a8bd2095ab255c832a42efcf5863f10814e7a1.css" />

    
    <script src="/js/highlight.min.min.872dfd2cd00064018a833a6e8e77a0fbf8fbac159546f2f205d4dad79a5d8e15.js"></script>
    <script>hljs.highlightAll();</script>

    <script>(()=>{var t=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches,e=localStorage.getItem("theme");t&&e===null&&(localStorage.setItem("theme","dark"),document.documentElement.setAttribute("data-dark-mode","")),t&&e==="dark"&&document.documentElement.setAttribute("data-dark-mode",""),e==="dark"&&document.documentElement.setAttribute("data-dark-mode","")})()</script>
    </head>
<body>
      <main class="wrapper"><nav class="navigation">
    <section class="container">
        <a class="navigation-brand" href="/">
            主页
        </a>
        <input type="checkbox" id="menu-toggle" />
        <label class="menu-button float-right" for="menu-toggle">
            <span></span><span></span><span></span>
        </label>
        
        <ul class="navigation-list" id="navigation-list">
            
            
            <li class="navigation-item navigation-menu">
                <a class="navigation-link" href="/blog/">文章</a>
            </li>
            
            <li class="navigation-item navigation-menu">
                <a class="navigation-link" href="/tags/">分类</a>
            </li>
            
            <li class="navigation-item navigation-menu">
                <a class="navigation-link" href="/archives/">历史文章</a>
            </li>
            
            

            <li class="navigation-item menu-separator">
                <span>|</span>
            </li>

            
            
            <li class="navigation-item navigation-social">
                <a class="navigation-link" href="https://github.com/p1um3r"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-github"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path></svg></a>
            </li>
            
            

            

            

            <li class="navigation-item navigation-dark">
                <button id="mode" type="button" aria-label="toggle user light or dark theme">
                    <span class="toggle-dark"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-moon"><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path></svg></span>
                    <span class="toggle-light"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-sun"><circle cx="12" cy="12" r="5"></circle><line x1="12" y1="1" x2="12" y2="3"></line><line x1="12" y1="21" x2="12" y2="23"></line><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line><line x1="1" y1="12" x2="3" y2="12"></line><line x1="21" y1="12" x2="23" y2="12"></line><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line></svg></span>
                </button>
            </li>

            
        </ul>
        
    </section>
</nav>
<div id="content">
<article class="blog-single">
  <header class="blog-title">
    <h1>深度学习学习笔记——数据操作</h1>
    <style type="text/css">
      ::-webkit-scrollbar {
        width: 8px;
        height: 8px;
      }
      ::-webkit-scrollbar-thumb {
        height: 40px;
        background-color: #eee;
        border-radius: 16px;
        &:hover {
          background-color: #ddd;
        }
      }
    </style>
  </header>

  <p>
  <small>
    2023年12月17日&nbsp;· 510 字&nbsp;· 3 分钟</small>

  
<p>

  <div class="blog-toc" style="position: fixed; left:50px;max-width:300px; overflow:auto; top: 100px; width: 15vw; bottom:50px">
    
<div class="post-toc" id="post-toc">
<aside>
    <header>
    <h3>深度学习学习笔记——数据操作</h3>
    </header>
    
    
    <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#1导入tensorflow">1、导入tensorflow</a></li>
        <li><a href="#2创建向量">2、创建向量</a></li>
        <li><a href="#3判断形状">3、判断形状</a></li>
        <li><a href="#4判断元素数量">4、判断元素数量</a></li>
        <li><a href="#5改变张量形状">5、改变张量形状</a></li>
        <li><a href="#6生成符合正态分布的随机取样均值为0标准差为1">6、生成符合正态分布的随机取样(均值为0、标准差为1)</a></li>
        <li><a href="#7生成定值张量">7、生成定值张量</a></li>
        <li><a href="#8基本运算">8、基本运算</a></li>
        <li><a href="#9张量连结concatenate">9、张量连结(concatenate)</a></li>
        <li><a href="#10求和">10、求和</a></li>
      </ul>
    </li>
    <li><a href="#机制">机制</a>
      <ul>
        <li><a href="#1广播机制">1、广播机制</a></li>
        <li><a href="#2索引和切片">2、索引和切片</a></li>
        <li><a href="#3节省内存">3、节省内存</a></li>
        <li><a href="#4转换为其他python对象">4、转换为其他python对象</a></li>
      </ul>
    </li>
  </ul>
</nav>
    
    
</aside>
<a href="#" id="toc-toggle"></a>
</div>



  </div>

  <section class="blog-content"><p>主要学习网站：https://zh.d2l.ai/</p>
<p>b站课程视频链接：https://space.bilibili.com/1567748478/channel/seriesdetail?sid=358497</p>
<p>视频和网站配合学习，一开始没看到有b站课程，只看网站学看不懂函数是干什么的，还不会问gpt，只是自己硬啃，确实有点吃力。。。</p>
<p>这里我学习的是tensorflow框架，并在jupyter中进行编写</p>
<p>前言什么的略过，主要记录代码方面学习到的内容。</p>
<h3 id="1导入tensorflow">1、导入tensorflow</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> tensorflow <span style="color:#66d9ef">as</span> tf
</span></span></code></pre></div><h3 id="2创建向量">2、创建向量</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>x <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>range(<span style="color:#ae81ff">12</span>)
</span></span><span style="display:flex;"><span>x
</span></span></code></pre></div><pre tabindex="0"><code>&lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11], dtype=int32)&gt;
</code></pre><h3 id="3判断形状">3、判断形状</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>x<span style="color:#f92672">.</span>shape
</span></span></code></pre></div><pre tabindex="0"><code>TensorShape([12])
</code></pre><h3 id="4判断元素数量">4、判断元素数量</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>tf<span style="color:#f92672">.</span>size(x)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">&lt;</span>tf<span style="color:#f92672">.</span>Tensor: shape<span style="color:#f92672">=</span>(), dtype<span style="color:#f92672">=</span>int32, numpy<span style="color:#f92672">=</span><span style="color:#ae81ff">12</span><span style="color:#f92672">&gt;</span>
</span></span></code></pre></div><h3 id="5改变张量形状">5、改变张量形状</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>X <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reshape(x, (<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>))
</span></span><span style="display:flex;"><span>X
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">&lt;</span>tf<span style="color:#f92672">.</span>Tensor: shape<span style="color:#f92672">=</span>(<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>), dtype<span style="color:#f92672">=</span>int32, numpy<span style="color:#f92672">=</span>
</span></span><span style="display:flex;"><span>array([[ <span style="color:#ae81ff">0</span>,  <span style="color:#ae81ff">1</span>,  <span style="color:#ae81ff">2</span>,  <span style="color:#ae81ff">3</span>],
</span></span><span style="display:flex;"><span>       [ <span style="color:#ae81ff">4</span>,  <span style="color:#ae81ff">5</span>,  <span style="color:#ae81ff">6</span>,  <span style="color:#ae81ff">7</span>],
</span></span><span style="display:flex;"><span>       [ <span style="color:#ae81ff">8</span>,  <span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">11</span>]], dtype<span style="color:#f92672">=</span>int32)<span style="color:#f92672">&gt;</span>
</span></span></code></pre></div><h3 id="6生成符合正态分布的随机取样均值为0标准差为1">6、生成符合正态分布的随机取样(均值为0、标准差为1)</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>tf<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal(shape<span style="color:#f92672">=</span>[<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>])
</span></span></code></pre></div><pre tabindex="0"><code class="language-pyhton" data-lang="pyhton">&lt;tf.Tensor: shape=(3, 4), dtype=float32, numpy=
array([[-1.9437447 ,  1.7360141 ,  0.03836465, -0.23741092],
       [-0.7939557 , -0.89138025,  0.17875476, -1.8298926 ],
       [ 1.0334245 , -0.06715401, -1.0667316 , -1.1286876 ]],
      dtype=float32)&gt;
</code></pre><h3 id="7生成定值张量">7、生成定值张量</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>tf<span style="color:#f92672">.</span>constant([[<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">3</span>], [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>], [<span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>]])
</span></span></code></pre></div><pre tabindex="0"><code>&lt;tf.Tensor: shape=(3, 4), dtype=int32, numpy=
array([[2, 1, 4, 3],
       [1, 2, 3, 4],
       [4, 3, 2, 1]], dtype=int32)&gt;
</code></pre><h3 id="8基本运算">8、基本运算</h3>
<p>对于任意具有相同形状的张量， 常见的标准算术运算符（<code>+</code>、<code>-</code>、<code>*</code>、<code>/</code>和<code>**</code>）都可以被升级为按元素运算。</p>
<p>还有<code>tf.exp(x)</code>也是按元素计算</p>
<h3 id="9张量连结concatenate">9、张量连结(concatenate)</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>X <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reshape(tf<span style="color:#f92672">.</span>range(<span style="color:#ae81ff">12</span>, dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>float32), (<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>))
</span></span><span style="display:flex;"><span>Y <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>constant([[<span style="color:#ae81ff">2.0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">3</span>], [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>], [<span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>]])
</span></span><span style="display:flex;"><span>tf<span style="color:#f92672">.</span>concat([X, Y], axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>), tf<span style="color:#f92672">.</span>concat([X, Y], axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span></code></pre></div><pre tabindex="0"><code>(&lt;tf.Tensor: shape=(6, 4), dtype=float32, numpy=
 array([[ 0.,  1.,  2.,  3.],
        [ 4.,  5.,  6.,  7.],
        [ 8.,  9., 10., 11.],
        [ 2.,  1.,  4.,  3.],
        [ 1.,  2.,  3.,  4.],
        [ 4.,  3.,  2.,  1.]], dtype=float32)&gt;,
 &lt;tf.Tensor: shape=(3, 8), dtype=float32, numpy=
 array([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],
        [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],
        [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]], dtype=float32)&gt;)
</code></pre><p>使用$tf.concat()$函数来实现，其中$[X,Y]$分别表示需要连结的两个张量，$X$在前，$Y$在后，参数$axis$代表的是在某个指定方向上进行连接，$0$为行，$1$为列</p>
<p>由于例子中使用的是二维张量，故只有$0$和$1$两个方向。若为三维张量，则还可能沿着$2$方向进行连结。</p>
<h3 id="10求和">10、求和</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>tf<span style="color:#f92672">.</span>reduce_sum(X)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">&lt;</span>tf<span style="color:#f92672">.</span>Tensor: shape<span style="color:#f92672">=</span>(), dtype<span style="color:#f92672">=</span>float32, numpy<span style="color:#f92672">=</span><span style="color:#ae81ff">66.0</span><span style="color:#f92672">&gt;</span>
</span></span></code></pre></div><h2 id="机制">机制</h2>
<h3 id="1广播机制">1、广播机制</h3>
<p>主要用于两个形状不同的张量进行按元素操作。</p>
<p><strong>注意：</strong></p>
<p><strong>形状可以不同，但维度必须相同，维度不同的两个张量不可以通过广播机制来进行按元素操作</strong></p>
<p>工作方式：</p>
<ol>
<li>通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状；</li>
<li>对生成的数组执行按元素操作。</li>
</ol>
<p>在大多数情况下，我们将沿着数组中长度为1的轴进行广播，如下例子：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>MXNET
</span></span><span style="display:flex;"><span>PYTORCH
</span></span><span style="display:flex;"><span>TENSORFLOW
</span></span><span style="display:flex;"><span>PADDLE
</span></span><span style="display:flex;"><span>a <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reshape(tf<span style="color:#f92672">.</span>range(<span style="color:#ae81ff">3</span>), (<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>b <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reshape(tf<span style="color:#f92672">.</span>range(<span style="color:#ae81ff">2</span>), (<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>))
</span></span><span style="display:flex;"><span>a, b
</span></span></code></pre></div><pre tabindex="0"><code>(&lt;tf.Tensor: shape=(3, 1), dtype=int32, numpy=
 array([[0],
        [1],
        [2]], dtype=int32)&gt;,
 &lt;tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[0, 1]], dtype=int32)&gt;)
</code></pre><p>这里$a$和$b$分别是$3×1$和$1×2$的矩阵，为了让二者相加，我们将两个矩阵广播为形状为$3×2$的矩阵，方法为：将矩阵$a$复制列，矩阵$b$复制行，然后再按元素相加。</p>
<pre tabindex="0"><code>a + b
</code></pre><pre tabindex="0"><code>&lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy=
array([[0, 1],
       [1, 2],
       [2, 3]], dtype=int32)&gt;
</code></pre><p>我们可以使用来查看广播后的结果：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>tf<span style="color:#f92672">.</span>broadcast_to(a, [<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">2</span>]), tf<span style="color:#f92672">.</span>broadcast_to(b, [<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">2</span>])
</span></span></code></pre></div><pre tabindex="0"><code>(&lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy=
 array([[0, 0],
        [1, 1],
        [2, 2]], dtype=int32)&gt;,
 &lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy=
 array([[0, 1],
        [0, 1],
        [0, 1]], dtype=int32)&gt;)
</code></pre><h3 id="2索引和切片">2、索引和切片</h3>
<p>与python索引和切片机制相同，0为第一个元素，-1为最后一个元素。</p>
<p>由于TensorFlow中的Tensors是不可变的，也不能被赋值，我们可以使用<strong>Variables</strong>来生成一个可以赋值的可变容器。==请记住，TensorFlow中的梯度不会通过Variable反向传播（？看不懂）==。</p>
<p>方法如下：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>X_var <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(X)
</span></span><span style="display:flex;"><span>X_var[<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>]<span style="color:#f92672">.</span>assign(<span style="color:#ae81ff">9</span>)
</span></span><span style="display:flex;"><span>X_var
</span></span></code></pre></div><pre tabindex="0"><code>&lt;tf.Variable &#39;Variable:0&#39; shape=(3, 4) dtype=float32, numpy=
array([[ 0.,  1.,  2.,  3.],
       [ 4.,  5.,  9.,  7.],
       [ 8.,  9., 10., 11.]], dtype=float32)&gt;
</code></pre><p>这里首先，生成了一个与X形状、元素相同$(3×4)$的<strong>可变容器</strong> X_var ， 然后将 X_var 中第2行，第3列的元素赋值为9，然后进行输出。</p>
<p>同时，我们也可以索引多个元素来为他们同时赋值，如：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>X_var <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(X)
</span></span><span style="display:flex;"><span>X_var[<span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">2</span>, :]<span style="color:#f92672">.</span>assign(tf<span style="color:#f92672">.</span>ones(X_var[<span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">2</span>,:]<span style="color:#f92672">.</span>shape, dtype <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>float32) <span style="color:#f92672">*</span> <span style="color:#ae81ff">12</span>)
</span></span><span style="display:flex;"><span>X_var
</span></span></code></pre></div><h3 id="3节省内存">3、节省内存</h3>
<p>在tensorfflow中，若我们使用Y = X + Y，我们就会发现python自动为新生成的Y分配了一个新的内存，因为计算过程如下：</p>
<p>Python首先计算Y + X，为结果分配新的内存，然后使Y指向内存中这个新的位置</p>
<p>我们可以用如下代码来进行验证：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>before <span style="color:#f92672">=</span> id(Y)
</span></span><span style="display:flex;"><span>Y <span style="color:#f92672">=</span> Y <span style="color:#f92672">+</span> X
</span></span><span style="display:flex;"><span>id(Y) <span style="color:#f92672">==</span> before
</span></span></code></pre></div><p>输出为：False</p>
<p>这样是不可取的，因为这会总是不必要的分配内存，而在机器学习中，我们可能有数百兆的参数，且在一秒内多次更新所有参数，这会极大的占用内存空间，产生浪费，且降低运行效率</p>
<p>同时，如果我们不能在同一位置进行更新，有些代码的数据引用仍会指向旧的地址，使得它在无意中引用旧的参数。</p>
<p>因此，我们可以使用Variable来永久保存一个状态</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>Z <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>Variable(tf<span style="color:#f92672">.</span>zeros_like(Y))
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;id(Z):&#39;</span>, id(Z))
</span></span><span style="display:flex;"><span>Z<span style="color:#f92672">.</span>assign(X <span style="color:#f92672">+</span> Y)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;id(Z):&#39;</span>, id(Z))
</span></span></code></pre></div><pre tabindex="0"><code>id(Z): 140172812493920
id(Z): 140172812493920
</code></pre><p>TensorFlow提供了<strong>tf.function</strong>修饰符， 将计算封装在TensorFlow图中，该图在运行前经过编译和优化。 这允许TensorFlow删除未使用的值，并复用先前分配的且不再需要的值。 这样可以最大限度地减少TensorFlow计算的内存开销。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a6e22e">@tf.function</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">computation</span>(X, Y):
</span></span><span style="display:flex;"><span>    Z <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>zeros_like(Y)  <span style="color:#75715e"># 这个未使用的值将被删除</span>
</span></span><span style="display:flex;"><span>    A <span style="color:#f92672">=</span> X <span style="color:#f92672">+</span> Y  <span style="color:#75715e"># 当不再需要时，分配将被复用</span>
</span></span><span style="display:flex;"><span>    B <span style="color:#f92672">=</span> A <span style="color:#f92672">+</span> Y
</span></span><span style="display:flex;"><span>    C <span style="color:#f92672">=</span> B <span style="color:#f92672">+</span> Y
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> C <span style="color:#f92672">+</span> Y
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>computation(X, Y)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a6e22e">@tf.function</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">computation</span>(X, Y):
</span></span><span style="display:flex;"><span>    Z <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>zeros_like(Y)  <span style="color:#75715e"># 这个未使用的值将被删除</span>
</span></span><span style="display:flex;"><span>    A <span style="color:#f92672">=</span> X <span style="color:#f92672">+</span> Y  <span style="color:#75715e"># 当不再需要时，分配将被复用</span>
</span></span><span style="display:flex;"><span>    B <span style="color:#f92672">=</span> A <span style="color:#f92672">+</span> Y
</span></span><span style="display:flex;"><span>    C <span style="color:#f92672">=</span> B <span style="color:#f92672">+</span> Y
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> C <span style="color:#f92672">+</span> Y
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>computation(X, Y)
</span></span></code></pre></div><h3 id="4转换为其他python对象">4、转换为其他python对象</h3>
<p>将深度学习框架定义的张量转换为NumPy张量**（ndarray）**很容易，反之也同样容易。 转换后的结果不共享内存。 这个小的不便实际上是非常重要的：当在CPU或GPU上执行操作的时候， 如果Python的NumPy包也希望使用相同的内存块执行其他操作，人们不希望停下计算来等它。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>A <span style="color:#f92672">=</span> X<span style="color:#f92672">.</span>numpy()
</span></span><span style="display:flex;"><span>B <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>constant(A)
</span></span><span style="display:flex;"><span>type(A), type(B)
</span></span></code></pre></div><pre tabindex="0"><code>(numpy.ndarray, tensorflow.python.framework.ops.EagerTensor)
</code></pre><p>要将大小为1的张量转换为Python标量，我们可以调用item函数或Python的内置函数。</p>
<pre tabindex="0"><code>a = tf.constant([3.5]).numpy()
a, a.item(), float(a), int(a)
</code></pre><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>(array([<span style="color:#ae81ff">3.5</span>], dtype<span style="color:#f92672">=</span>float32), <span style="color:#ae81ff">3.5</span>, <span style="color:#ae81ff">3.5</span>, <span style="color:#ae81ff">3</span>)
</span></span></code></pre></div></section>

  
  
  <div class="paginator">
    
    
    <a class="next" href="https://p1um3r.github.io/blog/rsa%E8%A7%A3%E9%A2%98%E6%80%BB%E7%BB%93/"><span>RSA解题总结</span>
      <svg class="icon" width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M3.77086 21.1546C11.0491 22.698 21.4339 21.7773 21.4339 16.3608V4.63375C21.4339 3.93962 21.3581 3.30535 21.1917 2.76787M3.77086 21.1546C1.9934 20.7777 0.973585 18.7264 1.08749 16.688C1.2668 13.479 1.15721 9.43135 1.00513 6.21507C0.87809 3.52811 3.12891 1.16316 5.51029 1.25008C9.76594 1.40542 15.377 1.20229 18.7912 1.00542C20.0864 0.930734 20.8406 1.63385 21.1917 2.76787M3.77086 21.1546C4.56586 21.4723 5.49168 21.7879 6.5 22.0658M21.1917 2.76787C23.1097 4.18217 23.13 12.4191 22.9004 16.3608C20.8478 24.0194 12.3061 23.6662 6.5 22.0658M21.1917 2.76787C21.7612 4.51192 22.7203 9.67216 22 16.3608C21.2797 23.0494 11.3665 22.9511 6.5 22.0658M12.055 9C12.711 9.61644 14.3679 10.997 15.9519 11.7966C16.0174 11.8297 16.0154 11.9753 15.9494 12.0063C14.945 12.4779 13.0706 13.9264 12.055 15M15.5556 11.9667C13.1345 12.0608 8 12 6 11" stroke="currentColor" stroke-linecap="round"/>
      </svg>
    </a>
    
  </div>
  

  

<div class="related-resources">
  
    
    
    
  
</div>


  
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
    integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
  
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
    integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
    integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"
    onload="renderMathInElement(document.body,
      {
        delimiters: [
          {left: '$$', right: '$$', display:true},
          {left: '$', right: '$', display:false},
          {left: '\\(', right: '\\)', display: false},
          {left: '\\[', right: '\\]', display: true}
        ]
      }
    );"></script>
</article>


        </div>
        <div id="fastSearch">
          <input id="searchInput" tabindex="0">
          <ul id="searchResults">
          </ul>
        </div>
        <script src="/js/fuse.js"></script> 
        <script src="/js/fastsearch.js"></script><footer class="footer">
  <p>&copy; 2023 <a href="https://p1um3r.github.io">p1um&#39;s blog</a>
    Powered by
    <a href="https://gohugo.io/" rel="noopener" target="_blank">Hugo️️</a>
    <a href="https://github.com/guangzhengli/hugo-theme-ladder" rel="noopener" target="_blank">Ladder</a>
️  </p>
</footer>

<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M10.5376 22.7916C11.0152 22.7207 22.5795 21.1781 22.0978 10.4211C22.0536 9.43274 21.9303 8.53367 21.7387 7.71865M10.5376 22.7916C16.876 22.3728 20.0969 19.8899 21.5383 16.9142M10.5376 22.7916C9.7707 22.9055 8.97982 22.8964 8.19743 22.7725M21.7387 7.71865C21.4988 6.69828 21.1518 5.80967 20.7188 5.04257M21.7387 7.71865C22.6022 10.1105 23.0542 13.7848 21.5383 16.9142M20.7188 5.04257C17.1684 -1.24629 7.83127 0.632493 4.27577 5.04257C2.88063 6.77451 -0.0433281 11.1668 1.38159 16.6571C2.27481 20.0988 5.17269 22.2936 8.19743 22.7725M20.7188 5.04257C22.0697 6.9404 24.0299 11.3848 22.3541 15.4153M21.5383 16.9142C21.8737 16.4251 22.1428 15.9235 22.3541 15.4153M8.19743 22.7725C12.1971 23.4683 20.6281 22.971 22.3541 15.4153M14 10.945C13.3836 10.289 12.003 8.63215 11.2034 7.04814C11.1703 6.98257 11.0247 6.98456 10.9937 7.05061C10.5221 8.05496 9.07362 9.92941 8 10.945M11.0333 7.44444C10.9392 9.86549 11 15 12 17" stroke="currentColor" stroke-linecap="round"/>
    </svg>
</a>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };
</script>

<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'Copy';

        function copyingDone() {
            copybutton.innerHTML = 'Copied';
            setTimeout(() => {
                copybutton.innerHTML = 'Copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });
        codeblock.parentNode.appendChild(copybutton);
    });
</script></main>
    </body>
  
  <script src="/main.min.6bb26b69159420159c74dc9e097b06a578ed2b68c701466a91a44a9632d851bd0af167a1b30012387b4c512b48ad9ad4d3394e04d77ae38d57e1920fe4ed34fe.js" integrity="sha512-a7JraRWUIBWcdNyeCXsGpXjtK2jHAUZqkaRKljLYUb0K8WehswASOHtMUStIrZrU0zlOBNd6441X4ZIP5O00/g==" crossorigin="anonymous" defer></script></html>
